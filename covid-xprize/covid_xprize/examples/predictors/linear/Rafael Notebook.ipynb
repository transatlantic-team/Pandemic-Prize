{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T15:22:25.476131Z",
     "start_time": "2020-12-17T15:22:25.459345Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T15:52:48.474322Z",
     "start_time": "2020-12-17T15:52:48.421690Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['figure.dpi']= 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T15:56:09.127374Z",
     "start_time": "2020-12-17T15:56:09.075414Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, fixed\n",
    "\n",
    "\n",
    "# Data\n",
    "# from transat.data import HYPOTHETICAL_SUBMISSION_DATE\n",
    "from transat.data.load import download_historical, load_historical\n",
    "from transat.data.split import split_historical\n",
    "from transat.data.transform import preprocess_historical_basic, dataframe_to_array\n",
    "\n",
    "# Metric\n",
    "from transat.metric import mae\n",
    "\n",
    "# Scenario/Simulation\n",
    "from transat.data.scenario import generate_scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T15:52:49.381661Z",
     "start_time": "2020-12-17T15:52:49.327611Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATEST_DATA_URL = 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv'\n",
    "NPI_COLUMNS = ['C1_School closing',\n",
    "               'C2_Workplace closing',\n",
    "               'C3_Cancel public events',\n",
    "               'C4_Restrictions on gatherings',\n",
    "               'C5_Close public transport',\n",
    "               'C6_Stay at home requirements',\n",
    "               'C7_Restrictions on internal movement',\n",
    "               'C8_International travel controls',\n",
    "               'H1_Public information campaigns',\n",
    "               'H2_Testing policy',\n",
    "               'H3_Contact tracing',\n",
    "               'H6_Facial Coverings']\n",
    "ID_COLUMNS = ['CountryName',\n",
    "              'RegionName',\n",
    "              'GeoID',\n",
    "              'Date']\n",
    "CASES_COLUMN = ['NewCases']\n",
    "NUM_PREV_DAYS_TO_INCLUDE = 6\n",
    "WINDOW_SIZE = 7\n",
    "TRAINING_LAST_DATE = '2020-10-31'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T15:52:49.814597Z",
     "start_time": "2020-12-17T15:52:49.770199Z"
    }
   },
   "outputs": [],
   "source": [
    "HYPOTHETICAL_SUBMISSION_DATE = np.datetime64(\"2020-09-30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T15:56:13.799620Z",
     "start_time": "2020-12-17T15:56:12.471182Z"
    }
   },
   "outputs": [],
   "source": [
    "download_historical()\n",
    "df = load_historical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CountryName', 'CountryCode', 'RegionName', 'RegionCode',\n",
       "       'Jurisdiction', 'Date', 'C1_School closing', 'C1_Flag',\n",
       "       'C2_Workplace closing', 'C2_Flag', 'C3_Cancel public events', 'C3_Flag',\n",
       "       'C4_Restrictions on gatherings', 'C4_Flag', 'C5_Close public transport',\n",
       "       'C5_Flag', 'C6_Stay at home requirements', 'C6_Flag',\n",
       "       'C7_Restrictions on internal movement', 'C7_Flag',\n",
       "       'C8_International travel controls', 'E1_Income support', 'E1_Flag',\n",
       "       'E2_Debt/contract relief', 'E3_Fiscal measures',\n",
       "       'E4_International support', 'H1_Public information campaigns',\n",
       "       'H1_Flag', 'H2_Testing policy', 'H3_Contact tracing',\n",
       "       'H4_Emergency investment in healthcare', 'H5_Investment in vaccines',\n",
       "       'H6_Facial Coverings', 'H6_Flag', 'H7_Vaccination policy', 'H7_Flag',\n",
       "       'M1_Wildcard', 'ConfirmedCases', 'ConfirmedDeaths', 'StringencyIndex',\n",
       "       'StringencyIndexForDisplay', 'StringencyLegacyIndex',\n",
       "       'StringencyLegacyIndexForDisplay', 'GovernmentResponseIndex',\n",
       "       'GovernmentResponseIndexForDisplay', 'ContainmentHealthIndex',\n",
       "       'ContainmentHealthIndexForDisplay', 'EconomicSupportIndex',\n",
       "       'EconomicSupportIndexForDisplay'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GeoID'] = df['CountryName'] + '__' + df['RegionName'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.array(1))==np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1==np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['NewCases'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-8aac10368fa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgeo_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mgdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeoID\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mall_case_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCASES_COLUMN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mall_npi_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNPI_COLUMNS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xprize/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2906\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2907\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2908\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2910\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xprize/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xprize/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['NewCases'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "data = df\n",
    "nb_lookback_days = 30\n",
    "# Create training data across all countries for predicting one day ahead\n",
    "x_samples = []\n",
    "y_samples = []\n",
    "data = data.dropna(subset=['ConfirmedCases'])\n",
    "geo_ids = data.GeoID.unique()\n",
    "for g in geo_ids:\n",
    "    gdf = data[data.GeoID == g]\n",
    "    all_case_data = np.array(gdf[CASES_COLUMN])\n",
    "    all_npi_data = np.array(gdf[NPI_COLUMNS])\n",
    "\n",
    "    # Create one sample for each day where we have enough data\n",
    "    # Each sample consists of cases and npis for previous nb_lookback_days\n",
    "    nb_total_days = len(gdf)\n",
    "    for d in range(nb_lookback_days, nb_total_days - 1):\n",
    "        x_cases = all_case_data[d - nb_lookback_days:d]\n",
    "\n",
    "        # Take negative of npis to support positive\n",
    "        # weight constraint in Lasso.\n",
    "        x_npis = -all_npi_data[d - nb_lookback_days:d]\n",
    "\n",
    "        # Flatten all input data so it fits Lasso input format.\n",
    "        x_sample = np.concatenate([x_cases.flatten(),\n",
    "                                   x_npis.flatten()])\n",
    "        y_sample = all_case_data[d + 1]\n",
    "        x_samples.append(x_sample)\n",
    "        y_samples.append(y_sample)\n",
    "\n",
    "x_samples = np.array(x_samples)\n",
    "y_samples = np.array(y_samples).flatten()\n",
    "\n",
    "\n",
    "# Split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_samples,\n",
    "                                                    y_samples,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, data, verbose=False):\n",
    "        \"\"\"\n",
    "            Train the model\n",
    "\n",
    "            :param data: Training data\n",
    "            :type data: pandas.DataFrame\n",
    "            :param verbose: Whether to show traces for debug (True) or run in quiet mode (False)\n",
    "            :type verbose: bool\n",
    "        \"\"\"\n",
    "        # Set number of past days to use to make predictions\n",
    "        nb_lookback_days = 30\n",
    "\n",
    "        # Create training data across all countries for predicting one day ahead\n",
    "        x_samples = []\n",
    "        y_samples = []\n",
    "        data = data.dropna(subset=['ConfirmedCases'])\n",
    "        geo_ids = data.GeoID.unique()\n",
    "        for g in geo_ids:\n",
    "            gdf = data[data.GeoID == g]\n",
    "            all_case_data = np.array(gdf[CASES_COLUMN])\n",
    "            all_npi_data = np.array(gdf[NPI_COLUMNS])\n",
    "\n",
    "            # Create one sample for each day where we have enough data\n",
    "            # Each sample consists of cases and npis for previous nb_lookback_days\n",
    "            nb_total_days = len(gdf)\n",
    "            for d in range(nb_lookback_days, nb_total_days - 1):\n",
    "                x_cases = all_case_data[d - nb_lookback_days:d]\n",
    "\n",
    "                # Take negative of npis to support positive\n",
    "                # weight constraint in Lasso.\n",
    "                x_npis = -all_npi_data[d - nb_lookback_days:d]\n",
    "\n",
    "                # Flatten all input data so it fits Lasso input format.\n",
    "                x_sample = np.concatenate([x_cases.flatten(),\n",
    "                                           x_npis.flatten()])\n",
    "                y_sample = all_case_data[d + 1]\n",
    "                x_samples.append(x_sample)\n",
    "                y_samples.append(y_sample)\n",
    "\n",
    "        x_samples = np.array(x_samples)\n",
    "        y_samples = np.array(y_samples).flatten()\n",
    "        \n",
    "        \n",
    "        # Split data into train and test sets\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_samples,\n",
    "                                                            y_samples,\n",
    "                                                            test_size=0.2,\n",
    "                                                            random_state=301)\n",
    "\n",
    "        # Create and train Lasso model.\n",
    "        # Set positive=True to enforce assumption that cases are positively correlated\n",
    "        # with future cases and npis are negatively correlated.\n",
    "        self._model = RandomForestRegressor(max_depth=10, max_leaf_nodes=50, n_estimators=10, criterion='mae')\n",
    "        # Fit model\n",
    "        self._model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "        # Evaluate model\n",
    "        train_preds = self._model.predict(x_train)\n",
    "        train_preds = np.maximum(train_preds, 0)  # Don't predict negative cases\n",
    "        if verbose:\n",
    "            print('Train MAE:', self.mae(train_preds, y_train))\n",
    "\n",
    "        test_preds = self._model.predict(x_test)\n",
    "        test_preds = np.maximum(test_preds, 0)  # Don't predict negative cases\n",
    "        if verbose:\n",
    "            print('Test MAE:', self.mae(test_preds, y_test))\n",
    "\n",
    "        # %%\n",
    "\n",
    "        # Inspect the learned feature coefficients for the model\n",
    "        # to see what features it's paying attention to.\n",
    "\n",
    "        # Give names to the features\n",
    "        x_col_names = []\n",
    "        for d in range(-nb_lookback_days, 0):\n",
    "            x_col_names.append('Day ' + str(d) + ' ' + CASES_COLUMN[0])\n",
    "        for d in range(-nb_lookback_days, 1):\n",
    "            for col_name in NPI_COLUMNS:\n",
    "                x_col_names.append('Day ' + str(d) + ' ' + col_name)\n",
    "        \n",
    "        if verbose:\n",
    "            print('Intercept', self._model.intercept_)\n",
    "\n",
    "        # Save model to file\n",
    "        self.save()\n",
    "\n",
    "        # Save used data\n",
    "        data_path = os.path.join(self._models_base_path, self._data_file)\n",
    "        data.to_csv(data_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
